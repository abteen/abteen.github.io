@inproceedings{oraby-etal-2019-curate,
    title = "\textbf{Curate and Generate: A Corpus and Method for Joint Control of Semantics and Style in Neural {NLG}}",
    author = "Oraby, Shereen  and
      Harrison, Vrindavan  and
      \textbf{Abteen Ebrahimi}  and
      Walker, Marilyn",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    publisher = "Association for Computational Linguistics",
    year = {2019}
   }

@inproceedings{harrison2020athena,
      title={\textbf{Athena: {C}onstructing {D}ialogues Dynamically with Discourse Constraints}. \textit{{P}reprint.}}, 
      author={Vrindavan Harrison and Juraj Juraska and Wen Cui and Lena Reed and Kevin K. Bowden and Jiaqi Wu and Brian Schwarzmann and \textbf{Abteen Ebrahimi} and Rishi Rajasekaran and Nikhil Varghese and Max Wechsler-Azen and Steve Whittaker and Jeffrey Flanigan and Marilyn Walker},
      year={2020},

}

@inproceedings{ebrahimi-kann-2021-adapt,
    title = "\textbf{How to Adapt Your Pretrained Multilingual Model to 1600 Languages}",
    author = "\textbf{Abteen Ebrahimi}  and
      Kann, Katharina",
    year = "2021",
    publisher = "Association for Computational Linguistics",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
}

@inproceedings{mager-etal-2021-findings,
    title = "\textbf{Findings of the {A}mericas{NLP} 2021 Shared Task on Open Machine Translation for Indigenous Languages of the {A}mericas}",
    author = "Mager, Manuel  and
      Oncevay, Arturo  and
      \textbf{Abteen Ebrahimi}  and
      Ortega, John  and
      Rios, Annette  and
      Fan, Angela  and
      Gutierrez-Vasques, Ximena  and
      Chiruzzo, Luis  and
      Gim{\'e}nez-Lugo, Gustavo  and
      Ramos, Ricardo  and
      Meza Ruiz, Ivan Vladimir  and
      Coto-Solano, Rolando  and
      Palmer, Alexis  and
      Mager-Hois, Elisabeth  and
      Chaudhary, Vishrav  and
      Neubig, Graham  and
      Vu, Ngoc Thang",
          booktitle = "Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas",
              publisher = "Association for Computational Linguistics",
              year = {2021}
}


@inproceedings{ebrahimi2021americasnli,
    title = "\textbf{{A}mericas{NLI}: {E}valuating {Z}ero-shot {N}atural {L}anguage {U}nderstanding of {P}retrained {M}ultilingual {M}odels in {T}ruly {L}ow-resource {L}anguages}",
    author = "\textbf{Abteen Ebrahimi}  and
      Mager, Manuel  and
      Oncevay, Arturo  and
      Chaudhary, Vishrav  and
      Chiruzzo, Luis  and
      Fan, Angela  and
      Ortega, John  and
      Ramos, Ricardo  and
      Rios, Annette  and
      Meza Ruiz, Ivan Vladimir  and
      Gim{\'e}nez-Lugo, Gustavo  and
      Mager, Elisabeth  and
      Neubig, Graham  and
      Palmer, Alexis  and
      Coto-Solano, Rolando  and
      Vu, Thang  and
      Kann, Katharina",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2022",
    publisher = "Association for Computational Linguistics",
    abstract = "Pretrained multilingual models are able to perform cross-lingual transfer in a zero-shot setting, even for languages unseen during pretraining. However, prior work evaluating performance on unseen languages has largely been limited to low-level, syntactic tasks, and it remains unclear if zero-shot learning of high-level, semantic tasks is possible for unseen languages. To explore this question, we present AmericasNLI, an extension of XNLI (Conneau et al., 2018) to 10 Indigenous languages of the Americas. We conduct experiments with XLM-R, testing multiple zero-shot and translation-based approaches. Additionally, we explore model adaptation via continued pretraining and provide an analysis of the dataset by considering hypothesis-only models. We find that XLM-R{'}s zero-shot performance is poor for all 10 languages, with an average performance of 38.48{\%}. Continued pretraining offers improvements, with an average accuracy of 43.85{\%}. Surprisingly, training on poorly translated data by far outperforms all other methods with an accuracy of 49.12{\%}.",
}

@inproceedings{kann-etal-2022-open,
    title = "\textbf{{O}pen-domain {D}ialogue {G}eneration: {W}hat {W}e {C}an {D}o, {C}annot {D}o, and {S}hould {D}o {N}ext}",
    author = "Katharina Kann  and
      \textbf{Abteen Ebrahimi}  and
      Joewie Koh  and
      Shiran Dudy  and
      Alessandro Roncone",
    booktitle = "Proceedings of the 4th Workshop on NLP for Conversational AI",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    abstract = "Human{--}computer conversation has long been an interest of artificial intelligence and natural language processing research. Recent years have seen a dramatic improvement in quality for both task-oriented and open-domain dialogue systems, and an increasing amount of research in the area. The goal of this work is threefold: (1) to provide an overview of recent advances in the field of open-domain dialogue, (2) to summarize issues related to ethics, bias, and fairness that the field has identified as well as typical errors of dialogue systems, and (3) to outline important future challenges. We hope that this work will be of interest to both new and experienced researchers in the area.",
}

@inproceedings{kann-etal-2022-machine,
    title = "\textbf{{M}achine {T}ranslation {B}etween {H}igh-resource {L}anguages in a {L}anguage {D}ocumentation {S}etting}",
    author = "Katharina Kann  and
      \textbf{Abteen Ebrahimi}  and
      Kristine Stenzel  and
      Alexis Palmer",
    booktitle = "Proceedings of the First Workshop on NLP Applications to Field Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "Association for Computational Linguistics",
    abstract = "Language documentation encompasses translation, typically into the dominant high-resource language in the region where the target language is spoken. To make data accessible to a broader audience, additional translation into other high-resource languages might be needed. Working within a project documenting Kotiria, we explore the extent to which state-of-the-art machine translation (MT) systems can support this second translation {--} in our case from Portuguese to English. This translation task is challenging for multiple reasons: (1) the data is out-of-domain with respect to the MT system{'}s training data, (2) much of the data is conversational, (3) existing translations include non-standard and uncommon expressions, often reflecting properties of the documented language, and (4) the data includes borrowings from other regional languages. Despite these challenges, existing MT systems perform at a usable level, though there is still room for improvement. We then conduct a qualitative analysis and suggest ways to improve MT between high-resource languages in a language documentation setting.",
}

@inproceedings{ebrahimi-etal-2023-meeting,
    title = "\textbf{Meeting the Needs of Low-Resource Languages: The Value of Automatic Alignments via Pretrained Models}",
    author = "\textbf{Abteen Ebrahimi}  and
      McCarthy, Arya D.  and
      Oncevay, Arturo  and
      Ortega, John E.  and
      Chiruzzo, Luis  and
      Gim{\'e}nez-Lugo, Gustavo  and
      Coto-Solano, Rolando  and
      Kann, Katharina",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    abstract = "Large multilingual models have inspired a new class of word alignment methods, which work well for the model{'}s pretraining languages. However, the languages most in need of automatic alignment are low-resource and, thus, not typically included in the pretraining data. In this work, we ask: How do modern aligners perform on unseen languages, and are they better than traditional methods? We contribute gold-standard alignments for Bribri{--}Spanish, Guarani{--}Spanish, Quechua{--}Spanish, and Shipibo-Konibo{--}Spanish. With these, we evaluate state-of-the-art aligners with and without model adaptation to the target language. Finally, we also evaluate the resulting alignments extrinsically through two downstream tasks: named entity recognition and part-of-speech tagging. We find that although transformer-based methods generally outperform traditional models, the two classes of approach remain competitive with each other.",
}

@inproceedings{ebrahimi-etal-2023-findings,
    title = "\textbf{Findings of the {A}mericas{NLP} 2023 Shared Task on Machine Translation into Indigenous Languages}",
    author = "\textbf{Abteen Ebrahimi}  and
      Mager, Manuel  and
      Rijhwani, Shruti  and
      Rice, Enora  and
      Oncevay, Arturo  and
      Baltazar, Claudia  and
      Cort{\'e}s, Mar{\'\i}a  and
      Monta{\~n}o, Cynthia  and
      Ortega, John E.  and
      Coto-solano, Rolando  and
      Cruz, Hilaria  and
      Palmer, Alexis  and
      Kann, Katharina",
    booktitle = "Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    abstract = "In this work, we present the results of the AmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages of the Americas. This edition of the shared task featured eleven language pairs, one of which {--} Chatino-Spanish {--} uses a newly collected evaluation dataset, consisting of professionally translated text from the legal domain. Seven teams participated in the shared task, with a total of 181 submissions. Additionally, we conduct a human evaluation of the best system outputs, and compare them to the best submissions from the prior shared task. We find that this analysis agrees with the quantitative measures used to rank submissions, which shows further improvements of 9.64 ChrF on average across all languages, when compared to the prior winning system.",
}


@InProceedings{pmlr-v220-ebrahimi22a,
  title = 	 {\textbf{{F}indings of the {S}econd {A}mericas{NLP} {C}ompetition on {S}peech-to-{T}ext {T}ranslation}},
  author =      {\textbf{Abteen Ebrahimi} and Mager, Manuel and Wiemerslage, Adam and Denisov, Pavel and Oncevay, Arturo and Liu, Danni and Koneru, Sai and Ugan, Enes Yavuz and Li, Zhaolin and Niehues, Jan and Romero, Monica and Torre, Ivan G and Alum\"{a}e, Tanel and Kong, Jiaming and Polezhaev, Sergey and Belousov, Yury and Chen, Wei-Rui and Sullivan, Peter and Adebara, Ife and Talafha, Bashar and Inciarte, Alcides Alcoba and Abdul-Mageed, Muhammad and Chiruzzo, Luis and Coto-Solano, Rolando and Cruz, Hilaria and Flores-Sol\'{o}rzano, Sof\'{i}a and L\'{o}pez, Aldo Andr\'{e}s Alvarez and Meza-Ruiz, Ivan and Ortega, John E. and Palmer, Alexis and Salazar, Rodolfo Joel Zevallos and Stenzel, Kristine and Vu, Thang and Kann, Katharina},
  booktitle = 	 {Proceedings of the NeurIPS 2022 Competitions Track},
  year = 	 {2022},
  editor = 	 {Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob},
  volume = 	 {220},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28 Nov--09 Dec},
  publisher =    {PMLR},
  abstract = 	 {Indigenous languages, including those from the Americas, have received very little attention from the machine learning (ML) and natural language processing (NLP) communities. To tackle the resulting lack of systems for these languages and the accompanying social inequalities affecting their speakers, we conduct the second AmericasNLP competition (and the first one in collaboration with NeurIPS), which is centered around speech-to-text translation systems for Indigenous languages of the Americas. The competition features three tasks – (1) automatic speech recognition, (2) text-based machine translation, and (3) speech-to-text translation – and two tracks: constrained and unconstrained. Five Indigenous languages are covered: Bribri, Guarani, Kotiria, Wa’ikhana, and Quechua. In this overview paper, we describe the tasks, tracks, and languages, introduce the baseline and participating systems, and end with a summary of ongoing and future challenges for the automatic translation of Indigenous languages.}
}

@inproceedings{ebrahimi-wense-2024-zero,
    title = {\textbf{Zero-Shot vs. Translation-Based Cross-Lingual Transfer: The Case of Lexical Gaps}},
    author = {\textbf{Abteen Ebrahimi}  and
      von der Wense, Katharina},
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    abstract = "Cross-lingual transfer can be achieved through two main approaches: zero-shot transfer or machine translation (MT). While the former has been the dominant approach, both have been shown to be competitive. In this work, we compare the current performance and long-term viability of these methods. We leverage lexical gaps to create a multilingual question answering dataset, which provides a difficult domain for evaluation. Both approaches struggle in this setting, though zero-shot transfer performs better, as current MT outputs are not specific enough for the task. Using oracle translation offers the best performance, showing that this approach can perform well long-term, however current MT quality is a bottleneck. We also conduct an exploratory study to see if humans produce translations sufficient for the task with only general instructions. We find this to be true for the majority of translators, but not all. This indicates that while translation has the potential to outperform zero-shot approaches, creating MT models that generate accurate task-specific translations may not be straightforward.",
}

@inproceedings{ebrahimi-etal-2024-findings,
    title = {\textbf{Findings of the {A}mericas{NLP} 2024 Shared Task on Machine Translation into Indigenous Languages}},
    author = {\textbf{Abteen Ebrahimi} and
      de Gibert, Ona  and
      Vazquez, Raul  and
      Coto-Solano, Rolando  and
      Denisov, Pavel  and
      Pugh, Robert  and
      Mager, Manuel  and
      Oncevay, Arturo  and
      Chiruzzo, Luis  and
      von der Wense, Katharina  and
      Rijhwani, Shruti},
    editor = "Mager, Manuel  and
      Ebrahimi, Abteen  and
      Rijhwani, Shruti  and
      Oncevay, Arturo  and
      Chiruzzo, Luis  and
      Pugh, Robert  and
      von der Wense, Katharina",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    abstract = "This paper presents the findings of the third iteration of the AmericasNLP Shared Task on Machine Translation. This year{'}s competition features eleven Indigenous languages found across North, Central, and South America. A total of six teams participate with a total of 157 submissions across all languages and models. Two baselines {--} the Sheffield and Helsinki systems from 2023 {--} are provided and represent hard-to-beat starting points for the competition. In addition to the baselines, teams are given access to a new repository of training data which consists of data collected by teams in prior shared tasks. Using ChrF++ as the main competition metric, we see improvements over the baseline for 4 languages: Chatino, Guarani, Quechua, and Rar{\'a}muri, with performance increases over the best baseline of 4.2 ChrF++. In this work, we present a summary of the submitted systems, results, and a human evaluation of system outputs for Bribri, which consists of both (1) a rating of meaning and fluency and (2) a qualitative error analysis of outputs from the best submitted system.",
}

@inproceedings{chiruzzo-etal-2024-findings,
    title = {\textbf{Findings of the {A}mericas{NLP} 2024 Shared Task on the Creation of Educational Materials for Indigenous Languages}},
    author = {Chiruzzo, Luis  and
      Denisov, Pavel  and
      Molina-Villegas, Alejandro  and
      Fernandez-Sabido, Silvia  and
      Coto-Solano, Rolando  and
      Ag{\"u}ero-Torales, Marvin  and
      Alvarez, Aldo  and
      Canul-Yah, Samuel  and
      Hau-Uc{\'a}n, Lorena  and
      \textbf{Abteen Ebrahimi}  and
      Pugh, Robert  and
      Oncevay, Arturo  and
      Rijhwani, Shruti  and
      von der Wense, Katharina  and
      Mager, Manuel},
    editor = "Mager, Manuel  and
      Ebrahimi, Abteen  and
      Rijhwani, Shruti  and
      Oncevay, Arturo  and
      Chiruzzo, Luis  and
      Pugh, Robert  and
      von der Wense, Katharina",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    abstract = "This paper presents the results of the first shared task about the creation of educational materials for three indigenous languages of the Americas.The task proposes to automatically generate variations of sentences according to linguistic features that could be used for grammar exercises.The languages involved in this task are Bribri, Maya, and Guarani.Seven teams took part in the challenge, submitting a total of 22 systems, obtaining very promising results.",
}

@inproceedings{ebrahimi2024scientificliteraturemultilingualmodels,
      title={\textbf{Since the Scientific Literature Is Multilingual, Our Models Should Be Too}}, 
      author={\textbf{Abteen Ebrahimi} and Kenneth Church},
      year={2024},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
publisher = {\textit{Preprint.}}
}

@inproceedings{church2024academicarticlerecommendationusing,
      title={\textbf{Academic Article Recommendation Using Multiple Perspectives}}, 
      author={Kenneth Church and Omar Alonso and Peter Vickers and Jiameng Sun and \textbf{Abteen Ebrahimi} and Raman Chandrasekar},
      year={2024},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
publisher = {\textit{Preprint.}}
}

@article{kann-americasnli,
AUTHOR={Kann, Katharina  and \textbf {Abteen Ebrahimi} and Mager, Manuel  and Oncevay, Arturo  and Ortega, John E.  and Rios, Annette  and Fan, Angela  and Gutierrez-Vasques, Ximena  and Chiruzzo, Luis  and Giménez-Lugo, Gustavo A.  and Ramos, Ricardo  and Meza Ruiz, Ivan Vladimir  and Mager, Elisabeth  and Chaudhary, Vishrav  and Neubig, Graham  and Palmer, Alexis  and Coto-Solano, Rolando  and Vu, Ngoc Thang },

TITLE="\textbf{AmericasNLI: Machine Translation and Natural Language Inference Systems for Indigenous Languages of the Americas}",

JOURNAL={Frontiers in Artificial Intelligence},

VOLUME={5},

YEAR={2022},

ISSN={2624-8212},
}